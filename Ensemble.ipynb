{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Ensemble.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMmH0hzAjyP9N611Mt0tbJ1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l-kKfpFJ2tu2","executionInfo":{"status":"ok","timestamp":1650192457688,"user_tz":-120,"elapsed":2022,"user":{"displayName":"Nikita Dilman","userId":"06419702465665096398"}},"outputId":"f36e28eb-1b81-4569-eb7b-b2db77aa7c6e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/AI/neurowood\n","ConvNeXt  data\tEnsemble.ipynb\tTraining.ipynb\tViT\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/AI/neurowood\n","!ls"]},{"cell_type":"code","source":["!pip install datasets transformers wandb timm==0.4.12 six tensorboardX -qq"],"metadata":{"id":"3Cn2kMCP3JCf","executionInfo":{"status":"ok","timestamp":1650192463439,"user_tz":-120,"elapsed":5753,"user":{"displayName":"Nikita Dilman","userId":"06419702465665096398"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["import argparse\n","import datetime\n","import numpy as np\n","import time\n","import torch\n","import torch.nn as nn\n","import torch.backends.cudnn as cudnn\n","import json\n","import os\n","from PIL import Image\n","from tqdm import tqdm\n","import csv\n","from pathlib import Path\n","\n","from timm.data.constants import \\\n","    IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n","from timm.models import create_model\n","import torchvision.transforms as T\n","\n","from transformers import ViTForImageClassification, AutoFeatureExtractor\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'"],"metadata":{"id":"x4s99OZ-BkxR","executionInfo":{"status":"ok","timestamp":1650192472104,"user_tz":-120,"elapsed":8668,"user":{"displayName":"Nikita Dilman","userId":"06419702465665096398"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def create_submit(data_path, model, tta_transforms=[]):\n","    test_images = os.listdir(data_path)\n","    labels = {}\n","    preds = []\n","    for test_image in tqdm(test_images):\n","        orig_image = Image.open(os.path.join(data_path, test_image))\n","        for transform in tta_transforms:\n","            image = transform(orig_image)\n","            output = model(image)\n","            preds.append(output)\n","        output = model(orig_image)\n","        preds.append(output)\n","        mn = torch.mean(torch.cat(preds, dim=0), dim=0)\n","        labels[int(test_image.split('.')[0])] = torch.argmax(mn)\n","    labels = {k: labels[k] for k in sorted(labels)}\n"," \n","    with open('submit.csv', 'w', encoding='UTF8') as f:\n","        writer = csv.writer(f)\n","        writer.writerow(['id', 'class'])\n","        for idx, label in labels.items():\n","            label = label.cpu().numpy()\n","            if label == 2:\n","                label = 3\n","            writer.writerow([idx, label])"],"metadata":{"id":"stH22V3sGaLB","executionInfo":{"status":"ok","timestamp":1650192553142,"user_tz":-120,"elapsed":240,"user":{"displayName":"Nikita Dilman","userId":"06419702465665096398"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["## ViT Fold Inference"],"metadata":{"id":"jH7OYQdjGPNE"}},{"cell_type":"code","source":["class ViTFold:\n","    def __init__(self, dev_path, labels):\n","        self.dev_path = dev_path\n","        self.labels = labels\n","        self.feature_extractor = AutoFeatureExtractor.from_pretrained(self.dev_path + '_fold1')\n","        self.fold1 = ViTForImageClassification.from_pretrained(self.dev_path + '_fold1',\n","            num_labels=len(self.labels),\n","            id2label={str(i): c for i, c in enumerate(self.labels)},\n","            label2id={c: str(i) for i, c in enumerate(self.labels)}\n","        )\n","        self.fold2 = ViTForImageClassification.from_pretrained(self.dev_path + '_fold2',\n","            num_labels=len(self.labels),\n","            id2label={str(i): c for i, c in enumerate(self.labels)},\n","            label2id={c: str(i) for i, c in enumerate(self.labels)}\n","        )\n","        self.fold3 = ViTForImageClassification.from_pretrained(self.dev_path + '_fold3',\n","            num_labels=len(self.labels),\n","            id2label={str(i): c for i, c in enumerate(self.labels)},\n","            label2id={c: str(i) for i, c in enumerate(self.labels)}\n","        )\n","        self.models = [self.fold1, self.fold2, self.fold3]\n","\n","    def __call__(self, image): \n","        image = self.feature_extractor(image, return_tensors='pt')['pixel_values']\n","        preds = []\n","        for model in self.models:\n","            preds.append(torch.softmax(model(image)[0], dim=1).cpu())\n","        \n","        output = torch.mean(torch.cat(preds, dim=0), dim=0)\n","        return output\n","\n","vit_model = ViTFold(dev_path='./ViT/vit-base-patch32-384', labels=[0, 1, 2])"],"metadata":{"id":"ZB3EjtVwrI0s","executionInfo":{"status":"ok","timestamp":1650192498519,"user_tz":-120,"elapsed":21188,"user":{"displayName":"Nikita Dilman","userId":"06419702465665096398"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["from torchvision import transforms\n","tta_transforms = [\n","    transforms.RandomHorizontalFlip(p=1),\n","    transforms.RandomVerticalFlip(p=1),\n","    transforms.RandomRotation(90),\n","    transforms.RandomRotation(180),\n","    transforms.RandomRotation(270)\n","]"],"metadata":{"id":"1CJEpQk2I-MU","executionInfo":{"status":"ok","timestamp":1650192542906,"user_tz":-120,"elapsed":233,"user":{"displayName":"Nikita Dilman","userId":"06419702465665096398"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["create_submit(data_path='data/test', model=vit_model, tta_transforms=tta_transforms)"],"metadata":{"id":"l0raCMVqJbw2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ConvNeXt Fold inference"],"metadata":{"id":"DnyAPMGYJcM-"}},{"cell_type":"code","source":["%cd ConvNeXt"],"metadata":{"id":"uryaB5RF3mj_","executionInfo":{"status":"ok","timestamp":1650192602676,"user_tz":-120,"elapsed":261,"user":{"displayName":"Nikita Dilman","userId":"06419702465665096398"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"522c5d32-71c0-42a1-ec6b-09d7830ccffa"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/AI/neurowood/ConvNeXt\n"]}]},{"cell_type":"code","source":["import models.convnext\n","import models.convnext_isotropic\n","\n","\n","class ConvFold:\n","    def __init__(self, dev_path, model_name, transforms):\n","        self.dev_path = dev_path\n","        self.transforms = transforms\n","        self.fold1 = create_model(\n","                model_name, \n","                pretrained=False, \n","                num_classes=3,\n","        )\n","        self.fold2 = create_model(\n","                model_name, \n","                pretrained=False, \n","                num_classes=3,\n","        )\n","        self.fold3 = create_model(\n","                model_name, \n","                pretrained=False, \n","                num_classes=3,\n","        )                \n","        \n","        self.models = [self.fold1, self.fold2, self.fold3]\n","        for idx, model in enumerate(self.models):\n","            model.load_state_dict(torch.load(dev_path + f'_fold{idx+1}.pth')['model'])\n","\n","    def __call__(self, image): \n","        image = self.transforms(image).unsqueeze(0).to(device)\n","        preds = []\n","        for model in self.models:\n","            model = model.to(device)\n","            preds.append(torch.softmax(model(image).cpu(), dim=1))\n","            model = model.to('cpu')\n","        output = torch.mean(torch.stack(preds, dim=0), dim=0)\n","        return output\n","\n","NORMALIZE_MEAN = IMAGENET_DEFAULT_MEAN\n","NORMALIZE_STD = IMAGENET_DEFAULT_STD\n","SIZE = 384\n","\n","orig_transforms = [\n","    T.Resize((SIZE, SIZE)),\n","    T.ToTensor(),\n","    T.Normalize(NORMALIZE_MEAN, NORMALIZE_STD),\n","]\n","\n","orig_transforms = T.Compose(orig_transforms)          \n","\n","conv_model = ConvFold('./checkpoints/convnext_base_1k_384', \"convnext_base\", transforms=orig_transforms)"],"metadata":{"id":"wsdrA1e5v6NY","executionInfo":{"status":"ok","timestamp":1650192632334,"user_tz":-120,"elapsed":27301,"user":{"displayName":"Nikita Dilman","userId":"06419702465665096398"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["create_submit(data_path='../data/test', model=conv_model, tta_transforms=tta_transforms)"],"metadata":{"id":"So1BRshf8kDP"},"execution_count":null,"outputs":[]}]}